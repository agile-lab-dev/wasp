version: '2'
services:
  mongo:
    image: mongo:3.0.2
    container_name: mongo
    ports:
     - "27017:27017"

  zookeeper:
    image: wurstmeister/zookeeper:3.4.6
    container_name: zookeeper
    ports:
      - "2182:2181"
    volumes:
      - "./zookeeper:/opt/zookeeper-3.4.6/conf"

  kafka:
    image: wurstmeister/kafka:0.10.2.1
    container_name: kafka
    environment:
      KAFKA_ADVERTISED_HOST_NAME: kafka
      KAFKA_ADVERTISED_PORT: 9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_BROKER_ID: 1
      KAFKA_SESSION_TIMEOUT_MS: 5000
    ports:
      - "9092:9092"
    entrypoint: [ # change the entrypoint of the image; this starts the boker only when its Zookeeper node for the id disappears
      "bash",
      "-c",
      "while [[ $$(echo get /brokers/ids/$$KAFKA_BROKER_ID | opt/kafka_2.12-0.10.2.1/bin/zookeeper-shell.sh zookeeper:2181 2>&1 | grep \"Node does not exist\" | wc -l) -eq 0 ]]; do echo \"Sleeping for $$((KAFKA_SESSION_TIMEOUT_MS/1000)) seconds waiting for the Zookeeper node for this broker (id $$KAFKA_BROKER_ID) to expire...\"; sleep $$((KAFKA_SESSION_TIMEOUT_MS/1000)); done; echo \"No Zookeeper node for this broker (id $$KAFKA_BROKER_ID) found, starting...\"; start-kafka.sh"]
    depends_on:
      - zookeeper

  spark-master:
    # Change to the spark version when gettyimages will put the image tag
    image: gettyimages/spark:2.2.0-hadoop-2.7
    command: ["bin/spark-class","org.apache.spark.deploy.master.Master","-h","spark-master"]
    container_name: spark-master
    environment:
      SPARK_CONF_DIR: /etc/spark
    expose:
      - "7001"
      - "7002"
      - "7003"
      - "7004"
      - "7005"
      - "7006"
    ports:
      - "6066:6066"
      - "7077:7077"
      - "8080:8080"
      - "18080:18080"
    volumes:
      - "./spark-master:/etc/spark"

  spark-worker:
    image: gettyimages/spark:2.2.0-hadoop-2.7
    command: ["bin/spark-class","org.apache.spark.deploy.worker.Worker","spark://spark-master:7077"]
    container_name: spark-worker
    environment:
      SPARK_CONF_DIR: /etc/spark
      SPARK_WORKER_CORES: 4
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_PORT: 8881
    expose:
      - "7012"
      - "7013"
      - "7014"
      - "7015"
      - "7016"
      - "8881"
    ports:
      - "8081:8081"
    volumes:
      - "./spark-worker:/etc/spark"
    depends_on:
      - spark-master

  elastickibana:
    image: devdb/kibana:latest
    container_name: elastickibana
    ports:
      - "5601:5601"
      - "9200:9200"
      - "9300:9300"
    volumes:
      - "./elasticsearch:/opt/elasticsearch/config"
      - "./kibana:/opt/kibana/config"

networks:
  default:
    external:
      name: wasp-docker
