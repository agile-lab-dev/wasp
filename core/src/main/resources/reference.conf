wasp {

  # framework-related configuration

  actor-system-name = "WASP"
  actor-downing-timeout-millis = 10000 # do not change unless you know what you're doing
  environment {
    validationRulesToIgnore = [] # array of validation-rule's keys, do not change unless you know what you're doing
    mode = "production" # production, develop
    prefix = ""  # should not contain space or /. chars
  }
  systempipegraphs.start = true # whether to automatically start system pipegraphs
  systemproducers.start = true # whether to automatically start system producers
  index-rollover = false
  general-timeout-millis = 60000
  services-timeout-millis = 15000

  rest {
    server {
      hostname = "localhost"
      port = 2891
    }
  }

  datastore {
    indexed = "solr"
    keyvalue = "hbase"
  }

  akka {
    loglevel = "DEBUG"
    loggers = ["akka.event.slf4j.Slf4jLogger"]
    logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
    logger-startup-timeout = 60s
    log-dead-letters = off
    log-dead-letters-during-shutdown = off

    remote {
      log-remote-lifecycle-events = off
      enabled-transports = ["akka.remote.netty.tcp"]
      netty.tcp {
        port = 2892
        hostname = "localhost"
      }
    }

    actor {
      provider = "akka.cluster.ClusterActorRefProvider"
    }

    cluster {
      log-info = on
      seed-nodes = ["akka.tcp://WASP@localhost:2892"]
      gossip-interval = 5s
      publish-stats-interval = 10s
      metrics.gossip-interval = 10s
      metrics.collect-interval = 10s
    }
  }


  # external-services-related configuration

  mongo {
    address = "mongodb://localhost:27017"
    db-name = "wasp"
    timeout = ${wasp.services-timeout-millis}
  }

  kafka {
    connections = [{
      protocol = ""
      host = "localhost"
      port = 9092
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    zookeeperConnections = [{
      protocol = ""
      host = "localhost"
      port = 2181
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    zkChRoot = "/kafka"
    ingest-rate = "1s"
    broker-id = 0
    partitioner-fqcn = "org.apache.kafka.clients.producer.internals.DefaultPartitioner"
    default-encoder = "kafka.serializer.DefaultEncoder"
    key-encoder-fqcn = "org.apache.kafka.common.serialization.StringSerializer"
    encoder-fqcn = "org.apache.kafka.common.serialization.ByteArraySerializer"
    decoder-fqcn = "org.apache.kafka.common.serialization.ByteArrayDeserializer"
    batch-send-size = 0
    acks = -1
    #others = [
    #  # mandatory
    #  { "security.protocol" = "SASL_PLAINTEXT" }
    #  { "sasl.kerberos.service.name" = "kafka" }
    #  { "sasl.jaas.config" = "com.sun.security.auth.module.Krb5LoginModule required storeKey=true useKeyTab=true useTicketCache=false keyTab=\"./wasp2.keytab\" serviceName=\"kafka\" principal=\"wasp2@REALM\";" }
    #  { "sasl.mechanism" = "GSSAPI" }
    #  { "kafka.security.protocol" = "SASL_PLAINTEXT" }
    #  { "kafka.sasl.kerberos.service.name" = "kafka" }
    #  { "kafka.sasl.jaas.config" = "com.sun.security.auth.module.Krb5LoginModule required storeKey=true useKeyTab=true useTicketCache=false keyTab=\"./wasp2.keytab\" serviceName=\"kafka\" principal=\"wasp2@REALM\";" }
    #  { "kafka.sasl.mechanism" = "GSSAPI" }
    #
    #  # optional
    #  { "sasl.kerberos.kinit.cmd" = "/usr/bin/kinit" }
    #  { "sasl.kerberos.min.time.before.relogin" = "60000" }
    #  { "sasl.kerberos.ticket.renew.jitter" = "0.05" }
    #  { "sasl.kerberos.ticket.renew.window.factor" = "0.8" }
    #  { "kafka.sasl.kerberos.kinit.cmd" = "/usr/bin/kinit" }
    #  { "kafka.sasl.kerberos.min.time.before.relogin" = "60000" }
    #  { "kafka.sasl.kerberos.ticket.renew.jitter" = "0.05" }
    #  { "kafka.sasl.kerberos.ticket.renew.window.factor" = "0.8" }
    #]
  }

  spark-streaming {
    app-name = "WASP-streaming"
    master {
      protocol = ""
      host = "local[*]"
      port = 0
    }
    driver-conf {
      submit-deploy-mode = "client"
      driver-cores = 1
      driver-memory = "1G"
      driver-hostname = "localhost"
      driver-bind-address = "0.0.0.0"
      driver-port = 0
    }
    executor-cores = 2
    executor-memory = "1G"
    executor-instances = 1
    additional-jars-path = "/root/wasp/lib"
    yarn-jar = "./spark-lib/spark-assembly.jar"
    block-manager-port = 0
    broadcast-port = 0
    fileserver-port = 0
    retained-stages-jobs = 100
    retained-tasks = 5000
    retained-jobs = 100
    retained-executions = 100
    retained-batches = 100
    kryo-serializer {
      enabled = true
      registrators = "" # comma-separated list of fully qualified names
      strict = false
    }

    streaming-batch-interval-ms = 1000
    checkpoint-dir = "/checkpoint"  # localDevelop: "file:///checkpoint" ->  consumers-spark-streaming container localFSPath, production: "/checkpoint" -> HDFS container path

    #others = [
    #  { "spark.yarn.dist.files" = "file:///root/configurations/wasp2.keytab,file:///root/configurations/sasl.jaas.config" }
    #  { "spark.executor.extraJavaOptions" = "-Djava.security.auth.login.config=./sasl.jaas.config" }
    #  { "spark.authenticate" = "true" }
    #]
  }

  spark-batch {
    app-name = "WASP-batch"
    master {
      protocol = ""
      host = "local[*]"
      port = 0
    }
    driver-conf {
      submit-deploy-mode = "client"
      driver-cores = 1
      driver-memory = "1G"
      driver-hostname = "localhost"
      driver-bind-address = "0.0.0.0"
      driver-port = 0
    }
    executor-cores = 2
    executor-memory = "1G"
    executor-instances = 1
    additional-jars-path = "/root/wasp/lib"
    yarn-jar = "./spark-lib/spark-assembly.jar"
    block-manager-port = 0
    broadcast-port = 0
    fileserver-port = 0
    retained-stages-jobs = 100
    retained-tasks = 5000
    retained-jobs = 100
    retained-executions = 100
    retained-batches = 100
    kryo-serializer {
      enabled = true
      registrators = "" # comma-separated custom-KryoRegistrator list of fully qualified names
      strict = false
    }
    #others = [
    #  { "spark.yarn.dist.files" = "file:///root/configurations/wasp2.keytab,file:///root/configurations/sasl.jaas.config" }
    #  { "spark.executor.extraJavaOptions" = "-Djava.security.auth.login.config=./sasl.jaas.config" }
    #  { "spark.authenticate" = "true" }
    #]
  }

  elastic {
    connections = [
      {
        protocol = ""
        host = "localhost"
        port = 9300
        timeout = ${wasp.services-timeout-millis}
        metadata = [
          {"connectiontype": "binary"}
        ]
      },
      {
        protocol = ""
        host = "localhost"
        port = 9200
        timeout = ${wasp.services-timeout-millis}
        metadata = [
          {"connectiontype": "rest"}
        ]
      }
    ]
  }

  solrcloud {
    zookeeperConnections = [{
      protocol = ""
      host = "localhost"
      port = 2181
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    zkChRoot = "/solr"
  }

  hbase {
    core-site-xml-path = "/etc/hadoop/conf/core-site.xml"
    hbase-site-xml-path = "/etc/hadoop/conf/hbase-site.xml"
  }

  jdbc {
    connections {
      #CONNECTION_NAME {
      #  url = "jdbc:oracle:thin://HOST:PORT/DB"
      #  user = "USER"
      #  password = "PSW"
      #  driverName = "DRIVER_FULLY_QUALIFIED_NAME"
      #}
      # #Examples
      #mysql {
      #  url = "jdbc:mysql://mysql:3306/test_db"
      #  user = "root"
      #  password = "psw"
      #  driverName = "com.mysql.jdbc.Driver"
      #}
      #oracle {
      #  url = "jdbc:oracle:thin://oracl:1521/ORCLCDB.localdomain"
      #  user = "user"
      #  password = "psw"
      #  driverName = "oracle.jdbc.driver.OracleDriver"
      #}
    }
  }
}