wasp {
  akka {
    cluster {
      roles = ["producers", "logger"]
    }
  }
}
wasp {

  backlogSizeAnalyzer {
    pipegraphs = [{
      pipegraphName = "TestConsoleWriterStructuredJSONPipegraph"
    }]
  }
  kafkaThroughput {
    topics = [{
      topicName: "test_json.topic"
      triggerIntervalMs: 1000
      windowSizeMs: 5000
      sendMessageEvery: 1
    }]
  }
  # framework-related configuration

  #actor-system-name =
  #actor-downing-timeout-millis =  # do not change unless you know what you're doing
  #environment {
  #  validationRulesToIgnore = [] # array of validation-rule's keys, do not change unless you know what you're doing
  #  mode = # production, develop
  #  prefix =  # should not contain space or /. chars
  #}
  systempipegraphs.start = false # whether to automatically start system pipegraphs
  systemproducers.start = false # whether to automatically start system producers
  #index-rollover =
  #general-timeout-millis =
  #services-timeout-millis =
  darwinConnector = "hbase" #possible value is hbase, postgres

  avroSchemaManager {
    wasp-manages-darwin-connectors-conf = true

    ##hbase-conf
    darwin {
      connector = "hbase"
      namespace = "AVRO"            #optional
      table = "SCHEMA_REPOSITORY"   #optional
      type = "cached_eager"
      #hbaseSite =    required if wasp-manages-darwin-connectors-conf= false
      #coreSite =     required if wasp-manages-darwin-connectors-conf= false
      #isSecure =     required if wasp-manages-darwin-connectors-conf= false
      #principal =    required if wasp-manages-darwin-connectors-conf= false
      #keytabPath =   required if wasp-manages-darwin-connectors-conf= false
    }

    ##postgres-conf
    #darwin {
      #table =        #optional
      #host =         required
      #db =           required
      #user =         required
      #password =     required

    #}
  }

  rest {
    server {
      hostname = ${HOSTNAME}
      #port =
    }
  }

#  telemetry{
#    writer = "default"
#    latency.sample-one-message-every = 100
#    topic = {
#      name = "telemetryichanged"
#      partitions = 3
#      replica = 1
#      others = [
#        {"batch.size" = "4096"}
#        {"acks" = "0" }
#      ]
#    }
#  }

  #telemetry{
    #writer =
    #latency.sample-one-message-every =
  #}

  #datastore {
  #  indexed =
  #  keyvalue =
  #}

  akka {
    #loglevel =
    #loggers =
    #logging-filter =
    #logger-startup-timeout =
    #log-dead-letters =
    #log-dead-letters-during-shutdown =

    #remote {
    #  log-remote-lifecycle-events =
    #  enabled-transports =
    #  netty.tcp {
    #    port =
    #    hostname = ${HOSTNAME}
    #  }
    #}

    #actor {
    #  provider =
    #}

    cluster {
      #log-info =
      seed-nodes = [${WASP_AKKA_SEED_NODE}]
      #gossip-interval =
      #publish-stats-interval =
      #metrics.gossip-interval =
      #metrics.collect-interval =
    }
  }


  # external-services-related configuration

  mongo {
    address = ${WASP_MONGO_DB_CONNECTION}
    #db-name =
    #timeout =
  }

  kafka {
    connections = [{
      protocol = ""
      host = ${WASP_KAFKA_HOSTNAME}
      port = ${WASP_KAFKA_PORT}
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    zookeeperConnections = [{
      protocol = ""
      host = ${WASP_ZOOKEEPER_HOSTNAME}
      port = ${WASP_ZOOKEEPER_PORT}
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    #zkChRoot =
    #ingest-rate =
    #broker-id =
    #partitioner-fqcn =
    #default-encoder =
    #key-encoder-fqcn =
    #encoder-fqcn =
    #decoder-fqcn =
    #batch-send-size =
    #acks =
    #others = [
    #  # mandatory
    #  { "security.protocol" = "SASL_PLAINTEXT" }
    #  { "sasl.kerberos.service.name" = "kafka" }
    #  { "sasl.jaas.config" = "com.sun.security.auth.module.Krb5LoginModule required storeKey=true useKeyTab=true useTicketCache=false keyTab=\"./wasp2.keytab\" serviceName=\"kafka\" principal=\"wasp2@REALM\";" }
    #  { "sasl.mechanism" = "GSSAPI" }
    #  { "kafka.security.protocol" = "SASL_PLAINTEXT" }
    #  { "kafka.sasl.kerberos.service.name" = "kafka" }
    #  { "kafka.sasl.jaas.config" = "com.sun.security.auth.module.Krb5LoginModule required storeKey=true useKeyTab=true useTicketCache=false keyTab=\"./wasp2.keytab\" serviceName=\"kafka\" principal=\"wasp2@REALM\";" }
    #  { "kafka.sasl.mechanism" = "GSSAPI" }
    #
    #  # optional
    #  { "sasl.kerberos.kinit.cmd" = "/usr/bin/kinit" }
    #  { "sasl.kerberos.min.time.before.relogin" = "60000" }
    #  { "sasl.kerberos.ticket.renew.jitter" = "0.05" }
    #  { "sasl.kerberos.ticket.renew.window.factor" = "0.8" }
    #  { "kafka.sasl.kerberos.kinit.cmd" = "/usr/bin/kinit" }
    #  { "kafka.sasl.kerberos.min.time.before.relogin" = "60000" }
    #  { "kafka.sasl.kerberos.ticket.renew.jitter" = "0.05" }
    #  { "kafka.sasl.kerberos.ticket.renew.window.factor" = "0.8" }
    #]
  }

  spark-streaming {
    #app-name =
    master {
#      protocol = "spark"
      host = "yarn"
#      port = 7077
    }
    driver-conf {
      submit-deploy-mode = "client"
      #driver-cores =
      #driver-memory =
      driver-hostname = streaming.example.com 
      driver-bind-address = streaming.example.com
      #driver-port =
      #
    }
    executor-cores = 1
    executor-memory = "512m"
    trigger-interval-ms = 1000
    #cores-max =           # used for Spark Standalone cluster manager (otherwise all available cores are assigned at the first one)
    #executor-instances =  # used only for Hadoop YARN cluster manager
    additional-jars-path = "/"
    yarn-jar = ${WASP_HDFS_YARN_JARS} 
    #block-manager-port =
    #broadcast-port =
    #fileserver-port =
    #retained-stages-jobs =
    #retained-tasks =
    #retained-jobs =
    #retained-executions =
    #retained-batches =
    kryo-serializer {
      #enabled =
      #registrators = # comma-separated custom-KryoRegistrator list of fully qualified names
      #  """
      #      it.agilelab.bigdata.wasp.whitelabel.consumers.spark.serializer.kryo.CustomKyroRegistrator1,
      #      it.agilelab.bigdata.wasp.whitelabel.consumers.spark.serializer.kryo.CustomKyroRegistrator2
      #  """,
      #strict =
    }

#    nifi-stateless {
#      bootstrap = ${WASP_NIFI_BOOTSTRAP}
#      system= ${WASP_NIFI_SYSTEM}
#      stateless= ${WASP_NIFI_STATELESS}
#      extensions= ${WASP_NIFI_EXTENSIONS}
#    }

    #streaming-batch-interval-ms =
    checkpoint-dir = "/user/root/checkpoint"

    others = [
      #{"spark.files": "file://"${WASP_HOME}"/lib/it.agilelab.wasp-spark-telemetry-plugin-2.20.0-234-add-a-way-to-start-singletons-services-inside-executors-develop-a-jmx-scraper-as-a-poc-SNAPSHOT.jar"}
      #{"spark.yarn.dist.files": "file://"${WASP_HOME}"/lib/it.agilelab.wasp-spark-telemetry-plugin-2.20.0-234-add-a-way-to-start-singletons-services-inside-executors-develop-a-jmx-scraper-as-a-poc-SNAPSHOT.jar"}
      #{"spark.executor.plugins": "it.agilelab.bigdata.wasp.spark.plugins.telemetry.TelemetryPlugin"}
      #{ "spark.executor.extraJavaOptions" = "-Dwasp.plugin.telemetry.collection-interval=\"1 second\" -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=20000" }
    ]

  }

  spark-batch {
    #app-name =
    master {
#      protocol = "spark"
      host = "yarn"
#      port = 7077
    }
    driver-conf {
      submit-deploy-mode = "client"
      #driver-cores =
      #driver-memory =
      driver-hostname = batch.example.com 
      driver-bind-address = batch.example.com

      #driver-port =
    }
    executor-cores = 2
    executor-memory = "2g"
    #cores-max =           # used for Spark Standalone cluster manager (otherwise all available cores are assigned at the first one)
    #executor-instances =  # used only for Hadoop YARN cluster manager
    additional-jars-path = "/"
    yarn-jar = ${WASP_HDFS_YARN_JARS} 
    #broadcast-port =
    #fileserver-port =
    #retained-stages-jobs =
    #retained-tasks =
    #retained-jobs =
    #retained-executions =
    #retained-batches =
    kryo-serializer {
      #enabled =
  #    registrators = # comma-separated custom-KryoRegistrator list of fully qualified names
  #      """
  #          it.agilelab.bigdata.wasp.whitelabel.consumers.spark.serializer.kryo.CustomKyroRegistrator1,
  #          it.agilelab.bigdata.wasp.whitelabel.consumers.spark.serializer.kryo.CustomKyroRegistrator2
  #      """,
      #strict =
    }
    #others = [
      #  { "spark.yarn.dist.files" = "file:///root/configurations/wasp2.keytab,file:///root/configurations/sasl.jaas.config" }
      #  { "spark.authenticate" = "true" }
    #]
  }

  elastic {
    connections = [
      {
        protocol = ""
        host = ${HOSTNAME}
        port = 9300
        timeout = ${wasp.services-timeout-millis}
        metadata = [
          {"connectiontype": "binary"}
        ]
      },
      {
        protocol = ""
        host = ${HOSTNAME}
        port = 9200
        timeout = ${wasp.services-timeout-millis}
        metadata = [
          {"connectiontype": "rest"}
        ]
      }
    ]
  }

  solrcloud {
    zookeeperConnections = [{
      protocol = ""
      host = ${WASP_ZOOKEEPER_HOSTNAME} 
      port = ${WASP_ZOOKEEPER_PORT} 
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    #zkChRoot =
  }

  #hbase {
  #  core-site-xml-path =
  #  hbase-site-xml-path =
  #}

  jdbc {
    connections {
      mysql {
        url = "jdbc:mysql://"${HOSTNAME}":3306/test_db"
        user = "root"
        password = "psw"
        driverName = "com.mysql.jdbc.Driver"
      }
    }
  }

  eventengine {

    eventPipegraph: {
      isSystem: true
      trigger-interval-ms = 1000
      eventStrategy: [
        {
          name: "streming_source_1"
          trigger-interval-ms = 1000
          reader: {
            modelName: "KafkaReader-123"
            datastoreModelName: "fake-data"
            modelType: "Kafka"
            options: [
              {
                key: "rate-limit"
                value: "200"
              }
            ]
          }

          writer: {
            modelName: "KafkaWriter-123"
            datastoreModelName: "event-topic"
            modelType: "Kafka"
            options: [
              {
                key: "replicas"
                value: "1"
              },
              {
                key: "partitions"
                value: "3"
              },
            ]
          }
          trigger: {
            eventRules: [
              {
                name: "HighTemperature"
                streamingSource = "streamingSource1",
                statement = "temperature > 100",
                typeExpression = "'TempControl'",
                severityExpression = "IF( temperature < 0, \"WARN\", \"CRITICAL\" )",
                sourceIdExpression = "name",

              },
              {
                name = "OddHighNumbers",
                streamingSource = "streamingSource2",
                statement = "someNumber > 75 AND someStuff == \"dispari\"",
                typeExpression = "'OddHighNumbers'",
                severityExpression = "IF( temperature < 150, \"LOW_TEMP\", \"HIGH_TEMP\" )",
                sourceIdExpression = "name"
              }
            ]
          }
        },
        {
          name: "streaming_source_2"
          trigger-interval-ms = 2000
          reader: {
            modelName: "KafkaReader-123"
            datastoreModelName: "fake-data"
            modelType: "Kafka"
            options: [
              {
                key: "rate-limit"
                value: "200"
              }
            ]
          }
          writer: {
            modelName: "KafkaWriter-123"
            datastoreModelName: "event-topic"
            modelType: "Kafka"
            options: [
              {
                key: "replicas"
                value: "1"
              },
              {
                key: "partitions"
                value: "3"
              },
            ]
          }
          trigger: {
            eventRules: [
              {
                name: "LowTemperature"
                streamingSource = "streamingSource2",
                statement = "temperature < 15",
                typeExpression = "'TempControl'",
                severityExpression = "IF( temperature != 0, \"WARN\", \"CRITICAL\" )",
                sourceIdExpression = "name",

              }
            ]
          }
        }
      ]
    }

    mailingPipegraph: {
      isSystem: false
      trigger-interval-ms = 5000
      writer: {
        modelName: "MailWriter-123"
        datastoreModelName: "nothing"
        modelType: "Mail"
        options: [
          {
            key: "mail.smtp.host"
            value: ""
          }
          {
            key: "mail.smtp.port"
            value: "25"
          },
          {
            key: "mail-from"
            value: "postmaster@example.org"
          },
          {
            key: "mail.smtp.auth"
            value: "false"
          },
          {
            key: "mail.smtp.ssl.enable"
            value: "false"
          },
          {
            key: "username"
            value: ""
          },
          {
            key: "password"
            value: ""
          },
        ]
      }
      mailingStrategy: [
        {
          name: "mail_strategy_0"
          trigger-interval-ms = 1000
          reader: {
            modelName: "KafkaReader-123"
            datastoreModelName: "event-topic"
            modelType: "Kafka"
            options: [
              {
                key: "rate-limit"
                value: "200"
              }
            ]
          }
          trigger: {
            enableAggregation: false
            mailRules: [
              {
                name: "CriticalTemperature"
                statement: "severity = 'CRITICAL'"
                subjectExpression: "CONCAT('CRITICAL TEMPERATURE REGISTERED BY: ', sourceId)"
                templatePath: "/velocitytemplates/template1.vm"
                recipient: {
                  mailTo: "postmaster@example.org"
                  mailCc: "postmaster@example.org"
                }
              },
              {
                name: "MailWithCompositeStatement"
                statement: "(severity = 'WARN' AND eventType = 'HighTemperature') OR (severity = 'LOW_TEMP')"
                subjectExpression: "CONCAT('Streaming Source: ', source, ', Source ID: ', sourceId)"
                templatePath: "/velocitytemplates/template2.vm"
                recipient: {
                  mailTo: "postmaster@example.com"
                  mailBcc: "postmaster@example.com"
                }
              }
            ]
          }
        }
      ]
    }
  }
}
