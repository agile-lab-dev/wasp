wasp {

  hdfs-homedir = "hdfs://"${HOSTNAME}":9000/user/root"

  # framework-related configuration

  #actor-system-name =
  #actor-downing-timeout-millis =  # do not change unless you know what you're doing
  #environment {
  #  validationRulesToIgnore = [] # array of validation-rule's keys, do not change unless you know what you're doing
  #  mode = # production, develop
  #  prefix =  # should not contain space or /. chars
  #}
  systempipegraphs.start = false # whether to automatically start system pipegraphs
  systemproducers.start = false # whether to automatically start system producers
  #index-rollover =
  #general-timeout-millis =
  #services-timeout-millis =
  darwinConnector = "hbase" #possible value is hbase, postgres

  avroSchemaManager {
    wasp-manages-darwin-connectors-conf = true

    ##hbase-conf
    darwin {
      namespace = "AVRO"            #optional
      table = "SCHEMA_REPOSITORY"   #optional
      type = "cached_eager"
      #hbaseSite =    required if wasp-manages-darwin-connectors-conf= false
      #coreSite =     required if wasp-manages-darwin-connectors-conf= false
      #isSecure =     required if wasp-manages-darwin-connectors-conf= false
      #principal =    required if wasp-manages-darwin-connectors-conf= false
      #keytabPath =   required if wasp-manages-darwin-connectors-conf= false
    }

    ##postgres-conf
    #darwin {
    #table =        #optional
    #host =         required
    #db =           required
    #user =         required
    #password =     required

    #}
  }

  rest {
    server {
      hostname = ${HOSTNAME}
      #port =
    }
  }

  #  telemetry{
  #    writer = "default"
  #    latency.sample-one-message-every = 100
  #    topic = {
  #      name = "telemetryichanged"
  #      partitions = 3
  #      replica = 1
  #      others = [
  #        {"batch.size" = "4096"}
  #        {"acks" = "0" }
  #      ]
  #    }
  #  }

  #telemetry{
  #writer =
  #latency.sample-one-message-every =
  #}

  #datastore {
  #  indexed =
  #  keyvalue =
  #}

  akka {
    #loglevel =
    #loggers =
    #logging-filter =
    #logger-startup-timeout =
    #log-dead-letters =
    #log-dead-letters-during-shutdown =

    #remote {
    #  log-remote-lifecycle-events =
    #  enabled-transports =
    #  netty.tcp {
    #    port =
    #    hostname = ${HOSTNAME}
    #  }
    #}

    #actor {
    #  provider =
    #}

    cluster {
      #log-info =
      #  seed-nodes = ["akka.tcp://WASP@"${HOSTNAME}":2892"]
      #gossip-interval =
      #publish-stats-interval =
      #metrics.gossip-interval =
      #metrics.collect-interval =
    }
  }


  # external-services-related configuration

  mongo {
    address = "mongodb://"${HOSTNAME}":27017"
    #db-name =
    #timeout =
  }

  kafka {
    connections = [{
      protocol = ""
      host = ${HOSTNAME}
      port = 9092
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    zookeeperConnections = [{
      protocol = ""
      host = ${HOSTNAME}
      port = 2181
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    #zkChRoot =
    #ingest-rate =
    #broker-id =
    #partitioner-fqcn =
    #default-encoder =
    #key-encoder-fqcn =
    #encoder-fqcn =
    #decoder-fqcn =
    #batch-send-size =
    #acks =
    #others = [
    #  # mandatory
    #  { "security.protocol" = "SASL_PLAINTEXT" }
    #  { "sasl.kerberos.service.name" = "kafka" }
    #  { "sasl.jaas.config" = "com.sun.security.auth.module.Krb5LoginModule required storeKey=true useKeyTab=true useTicketCache=false keyTab=\"./wasp2.keytab\" serviceName=\"kafka\" principal=\"wasp2@REALM\";" }
    #  { "sasl.mechanism" = "GSSAPI" }
    #  { "kafka.security.protocol" = "SASL_PLAINTEXT" }
    #  { "kafka.sasl.kerberos.service.name" = "kafka" }
    #  { "kafka.sasl.jaas.config" = "com.sun.security.auth.module.Krb5LoginModule required storeKey=true useKeyTab=true useTicketCache=false keyTab=\"./wasp2.keytab\" serviceName=\"kafka\" principal=\"wasp2@REALM\";" }
    #  { "kafka.sasl.mechanism" = "GSSAPI" }
    #
    #  # optional
    #  { "sasl.kerberos.kinit.cmd" = "/usr/bin/kinit" }
    #  { "sasl.kerberos.min.time.before.relogin" = "60000" }
    #  { "sasl.kerberos.ticket.renew.jitter" = "0.05" }
    #  { "sasl.kerberos.ticket.renew.window.factor" = "0.8" }
    #  { "kafka.sasl.kerberos.kinit.cmd" = "/usr/bin/kinit" }
    #  { "kafka.sasl.kerberos.min.time.before.relogin" = "60000" }
    #  { "kafka.sasl.kerberos.ticket.renew.jitter" = "0.05" }
    #  { "kafka.sasl.kerberos.ticket.renew.window.factor" = "0.8" }
    #]
  }

  spark-streaming {
    #app-name =
    master {
      #      protocol = "spark"
      host = "yarn"
      #      port = 7077
    }
    driver-conf {
      submit-deploy-mode = "client"
      #driver-cores =
      #driver-memory =
      driver-hostname = ${HOSTNAME}
      driver-bind-address = ${HOSTNAME}
      #driver-port =
      #
    }
    executor-cores = 2
    executor-memory = "2g"
    executor-instances =  1
    #cores-max =           # used for Spark Standalone cluster manager (otherwise all available cores are assigned at the first one)
    #executor-instances =  # used only for Hadoop YARN cluster manager
    additional-jars-path = "/code/consumers-spark/lib"
    yarn-jar = "hdfs://"${HOSTNAME}":9000/user/root/spark2/lib/*"
    #block-manager-port =
    #broadcast-port =
    #fileserver-port =
    #retained-stages-jobs =
    #retained-tasks =
    #retained-jobs =
    #retained-executions =
    #retained-batches =
    kryo-serializer {
      #enabled =
      registrators = # comma-separated custom-KryoRegistrator list of fully qualified names
        "",
      #strict =
    }

    #streaming-batch-interval-ms =
    checkpoint-dir = "/user/root/checkpoint"

    others = [
      { "spark.yarn.stagingDir" = "hdfs://"${HOSTNAME}":9000/user/root/.sparkStaging/" }
      #{"spark.files": "file://"${WASP_HOME}"/lib/it.agilelab.wasp-spark-telemetry-plugin-2.20.0-234-add-a-way-to-start-singletons-services-inside-executors-develop-a-jmx-scraper-as-a-poc-SNAPSHOT.jar"}
      #{"spark.yarn.dist.files": "file://"${WASP_HOME}"/lib/it.agilelab.wasp-spark-telemetry-plugin-2.20.0-234-add-a-way-to-start-singletons-services-inside-executors-develop-a-jmx-scraper-as-a-poc-SNAPSHOT.jar"}
      #{"spark.executor.plugins": "it.agilelab.bigdata.wasp.spark.plugins.telemetry.TelemetryPlugin"}
      #{ "spark.executor.extraJavaOptions" = "-Dwasp.plugin.telemetry.collection-interval=\"1 second\"" }
    ]

  }

  spark-batch {
    #app-name =
    master {
      #      protocol = "spark"
      host = "yarn"
      #      port = 7077
    }
    driver-conf {
      submit-deploy-mode = "client"
      #driver-cores =
      #driver-memory =
      driver-hostname = ${HOSTNAME}
      driver-bind-address = ${HOSTNAME}
      #driver-port =
    }
    executor-cores = 2
    executor-memory = "1g"
    #cores-max =           # used for Spark Standalone cluster manager (otherwise all available cores are assigned at the first one)
    executor-instances =  1
    additional-jars-path = "/code/consumers-spark/lib"
    yarn-jar = "hdfs://"${HOSTNAME}":9000/user/root/spark2/lib/*"
    #block-manager-port =
    #broadcast-port =
    #fileserver-port =
    #retained-stages-jobs =
    #retained-tasks =
    #retained-jobs =
    #retained-executions =
    #retained-batches =
    kryo-serializer {
      #enabled =
      registrators = # comma-separated custom-KryoRegistrator list of fully qualified names
        "",
      #strict =
    }
    others = [
      #  { "spark.yarn.dist.files" = "file:///root/configurations/wasp2.keytab,file:///root/configurations/sasl.jaas.config" }
      { "spark.yarn.stagingDir" = "hdfs://"${HOSTNAME}":9000/user/root/.sparkStaging/" }
    ]

  }

  elastic {
    connections = [
      {
        protocol = ""
        host = ${HOSTNAME}
        port = 9300
        timeout = ${wasp.services-timeout-millis}
        metadata = [
          {"connectiontype": "binary"}
        ]
      },
      {
        protocol = ""
        host = ${HOSTNAME}
        port = 9200
        timeout = ${wasp.services-timeout-millis}
        metadata = [
          {"connectiontype": "rest"}
        ]
      }
    ]
  }

  solrcloud {
    zookeeperConnections = [{
      protocol = ""
      host = ${HOSTNAME}
      port = 2181
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    #zkChRoot =
  }

  hbase {
    core-site-xml-path = "/templates/core-site.xml"
    hbase-site-xml-path = "/etc/hbase/conf.dist/hbase-site.xml"
  }

  jdbc {
    connections {
      mysql {
        url = "jdbc:mysql://"${HOSTNAME}":3306/test_db"
        user = "root"
        password = "psw"
        driverName = "com.mysql.jdbc.Driver"
      }
    }
  }
}