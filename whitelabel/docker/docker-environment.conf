wasp {

  # framework-related configuration

  #actor-system-name =
  #actor-downing-timeout-millis =  # do not change unless you know what you're doing
  #environment {
  #  validationRulesToIgnore = [] # array of validation-rule's keys, do not change unless you know what you're doing
  #  mode = # production, develop
  #  prefix =  # should not contain space or /. chars
  #}
  systempipegraphs.start = false # whether to automatically start system pipegraphs
  systemproducers.start = false # whether to automatically start system producers
  #index-rollover =
  #general-timeout-millis =
  #services-timeout-millis =

  rest {
    server {
      hostname = "master"
      #port =
    }
  }

  #datastore {
  #  indexed =
  #  keyvalue =
  #}

  akka {
    #loglevel =
    #loggers =
    #logging-filter =
    #logger-startup-timeout =
    #log-dead-letters =
    #log-dead-letters-during-shutdown =

    #remote {
    #  log-remote-lifecycle-events =
    #  enabled-transports =
    #  netty.tcp {
    #    port =
    #    hostname =
    #  }
    #}

    #actor {
    #  provider =
    #}

    cluster {
      #log-info =
      seed-nodes = ["akka.tcp://WASP@master:2892"]
      #gossip-interval =
      #publish-stats-interval =
      #metrics.gossip-interval =
      #metrics.collect-interval =
    }
  }


  # external-services-related configuration

  mongo {
    address = "mongodb://mongo:27017"
    #db-name =
    #timeout =
  }

  kafka {
    connections = [{
      protocol = ""
      host = "kafka"
      port = 9092
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    zookeeperConnections = [{
      protocol = ""
      host = "zookeeper"
      port = 2181
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    #zkChRoot =
    #ingest-rate =
    #broker-id =
    #partitioner-fqcn =
    #default-encoder =
    #key-encoder-fqcn =
    #encoder-fqcn =
    #decoder-fqcn =
    #batch-send-size =
    #acks =
    #others = [
    #  # mandatory
    #  { "security.protocol" = "SASL_PLAINTEXT" }
    #  { "sasl.kerberos.service.name" = "kafka" }
    #  { "sasl.jaas.config" = "com.sun.security.auth.module.Krb5LoginModule required storeKey=true useKeyTab=true useTicketCache=false keyTab=\"./wasp2.keytab\" serviceName=\"kafka\" principal=\"wasp2@REALM\";" }
    #  { "sasl.mechanism" = "GSSAPI" }
    #  { "kafka.security.protocol" = "SASL_PLAINTEXT" }
    #  { "kafka.sasl.kerberos.service.name" = "kafka" }
    #  { "kafka.sasl.jaas.config" = "com.sun.security.auth.module.Krb5LoginModule required storeKey=true useKeyTab=true useTicketCache=false keyTab=\"./wasp2.keytab\" serviceName=\"kafka\" principal=\"wasp2@REALM\";" }
    #  { "kafka.sasl.mechanism" = "GSSAPI" }
    #
    #  # optional
    #  { "sasl.kerberos.kinit.cmd" = "/usr/bin/kinit" }
    #  { "sasl.kerberos.min.time.before.relogin" = "60000" }
    #  { "sasl.kerberos.ticket.renew.jitter" = "0.05" }
    #  { "sasl.kerberos.ticket.renew.window.factor" = "0.8" }
    #  { "kafka.sasl.kerberos.kinit.cmd" = "/usr/bin/kinit" }
    #  { "kafka.sasl.kerberos.min.time.before.relogin" = "60000" }
    #  { "kafka.sasl.kerberos.ticket.renew.jitter" = "0.05" }
    #  { "kafka.sasl.kerberos.ticket.renew.window.factor" = "0.8" }
    #]
  }

  spark-streaming {
    #app-name =
    master {
      protocol = "spark"
      host = "spark-master"
      port = 7077
    }
    driver-conf {
      #submit-deploy-mode =
      #driver-cores =
      #driver-memory =
      driver-hostname = "consumers-spark-streaming"
      #driver-bind-address =
      #driver-port =
    }
    #executor-cores =
    #executor-memory =
    #cores-max =           # used for Spark Standalone cluster manager (otherwise all available cores are assigned at the first one)
    #executor-instances =  # used only for Hadoop YARN cluster manager
    #additional-jars-path =
    #yarn-jar =
    #block-manager-port =
    #broadcast-port =
    #fileserver-port =
    #retained-stages-jobs =
    #retained-tasks =
    #retained-jobs =
    #retained-executions =
    #retained-batches =
    kryo-serializer {
      #enabled =
      registrators = # comma-separated custom-KryoRegistrator list of fully qualified names
        """
            it.agilelab.bigdata.wasp.whitelabel.consumers.spark.serializer.kryo.CustomKyroRegistrator1,
            it.agilelab.bigdata.wasp.whitelabel.consumers.spark.serializer.kryo.CustomKyroRegistrator2
        """,
      #strict =
    }

    #streaming-batch-interval-ms =
    #checkpoint-dir =

    #others = [
      #  { "spark.yarn.dist.files" = "file:///root/configurations/wasp2.keytab,file:///root/configurations/sasl.jaas.config" }
      #  { "spark.executor.extraJavaOptions" = "-Djava.security.auth.login.config=./sasl.jaas.config" }
      #  { "spark.authenticate" = "true" }
    #]
  }

  spark-batch {
    #app-name =
    master {
      protocol = "spark"
      host = "spark-master"
      port = 7077
    }
    driver-conf {
      #submit-deploy-mode =
      #driver-cores =
      #driver-memory =
      driver-hostname = "consumers-spark-batch"
      #driver-bind-address =
      #driver-port =
    }
    #executor-cores =
    #executor-memory =
    #cores-max =           # used for Spark Standalone cluster manager (otherwise all available cores are assigned at the first one)
    #executor-instances =  # used only for Hadoop YARN cluster manager
    #additional-jars-path =
    #yarn-jar =
    #block-manager-port =
    #broadcast-port =
    #fileserver-port =
    #retained-stages-jobs =
    #retained-tasks =
    #retained-jobs =
    #retained-executions =
    #retained-batches =
    kryo-serializer {
      #enabled =
      registrators = # comma-separated custom-KryoRegistrator list of fully qualified names
        """
            it.agilelab.bigdata.wasp.whitelabel.consumers.spark.serializer.kryo.CustomKyroRegistrator1,
            it.agilelab.bigdata.wasp.whitelabel.consumers.spark.serializer.kryo.CustomKyroRegistrator2
        """,
      #strict =
    }
    #others = [
      #  { "spark.yarn.dist.files" = "file:///root/configurations/wasp2.keytab,file:///root/configurations/sasl.jaas.config" }
      #  { "spark.executor.extraJavaOptions" = "-Djava.security.auth.login.config=./sasl.jaas.config" }
      #  { "spark.authenticate" = "true" }
    #]
  }

  elastic {
    connections = [
      {
        protocol = ""
        host = "elasticsearch"
        port = 9300
        timeout = ${wasp.services-timeout-millis}
        metadata = [
          {"connectiontype": "binary"}
        ]
      },
      {
        protocol = ""
        host = "elasticsearch"
        port = 9200
        timeout = ${wasp.services-timeout-millis}
        metadata = [
          {"connectiontype": "rest"}
        ]
      }
    ]
  }

  solrcloud {
    zookeeperConnections = [{
      protocol = ""
      host = "zookeeper"
      port = 2181
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    #zkChRoot =
  }

  #hbase {
  #  core-site-xml-path =
  #  hbase-site-xml-path =
  #}

  jdbc {
    connections {
      mysql {
        url = "jdbc:mysql://mysql:3306/test_db"
        user = "root"
        password = "psw"
        driverName = "com.mysql.jdbc.Driver"
      }
    }
  }
}